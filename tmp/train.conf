accum_count:
- 1
accum_steps:
- 0
adagrad_accumulator_init: 0
adam_beta1: 0.9
adam_beta2: 0.98
alignment_heads: 0
alignment_layer: -3
apex_opt_level: O1
attention_dropout:
- 0.3
audio_enc_pooling: '1'
average_decay: 0
average_every: 1
batch_size: 2
batch_type: sents
bidir_edges: true
bridge_extra_node: true
cnn_kernel_width: 3
data: tmp/ECB
data_ids:
- null
data_to_noise:
- null
data_weights:
- 1
dec_layers: 2
dec_rnn_size: 500
decay_method: noam
decay_steps: 8
decoder_type: transformer
dropout:
- 0.3
dropout_steps:
- 0
early_stopping: 10
enc_layers: 2
enc_rnn_size: 500
encoder_type: transformer_flat
epochs: 0
exp: ''
exp_host: ''
flat_layers: 5
feat_merge: concat
feat_vec_exponent: 0.7
feat_vec_size: -1
generator_function: softmax
global_attention: general
global_attention_function: softmax
gpu_backend: nccl
gpu_ranks:
- 0
gpu_verbose_level: 0
gpuid: []
heads: 8
image_channel_size: 3
input_feed: 1
keep_checkpoint: -1
label_smoothing: 0.1
lambda_align: 0.0
lambda_coverage: 0.0
layers: 6
learning_rate: 1.0
learning_rate_decay: 0.8
log_file: tmp/train.log
log_file_level: '0'
loss_scale: 0
master_ip: localhost
master_port: 10000
max_generator_batches: 32
max_grad_norm: 5
max_relative_positions: 0
model_dtype: fp32
model_type: text
n_edge_types: 2
n_node: 2
n_steps: 2
normalization: sents
optim: adam
param_init: 0.1
pool_factor: 8192
position_encoding: 'true'
queue_size: 40
report_every: 1
reset_optim: none
rnn_size: 512
rnn_type: LSTM
sample_rate: 16000
save_checkpoint_steps: 10000
save_model: tmp/model
seed: -1
segment_embedding: 'true'
self_attn_type: scaled-dot
share_embeddings: 'true'
src_vocab: ''
src_word_vec_size: 500
start_decay_steps: 50000
state_dim: 512
tensorboard_log_dir: runs/onmt
tgt_word_vec_size: 500
train_from: ''
train_steps: 300000
transformer_ff: 2048
truncated_decoder: 0
valid_batch_size: 32
valid_steps: 10000
warmup_steps: 4000
window_size: 0.02
word_vec_size: 512
world_size: 1
